# Document Question Answering with RAG

## Overview
This project demonstrates building a **Retrieval-Augmented Generation (RAG)** system to answer questions from PDF or TXT documents using embeddings and a local LLM.

---

## Steps Explanation

### 1. Install Libraries
Installs all the required Python libraries for document loading, text splitting, embeddings, vector databases, and LLMs.

### 2. Import Libraries
Imports all necessary classes and functions to handle document processing, embeddings, retrieval, prompts, and language model generation.

### 3. Upload Document
Allows the user to upload a PDF or TXT file and loads its content into Python for processing.

### 4. Split Document into Chunks
Breaks the document into smaller pieces (chunks) to ensure the LLM can handle it efficiently and to improve retrieval accuracy.

### 5. Create Embeddings and Vector Database
Converts each chunk into a numerical representation (embedding) and stores them in a vector database for semantic search and fast retrieval.

### 6. Load Local LLM
Loads a text-to-text language model (FLAN-T5) to generate answers from the retrieved document chunks.

### 7. Create RAG Chain
Combines retrieval, formatting, prompting, and LLM generation into a single pipeline that can answer questions using the document context.

### 8. Ask Questions
Provides an interactive interface where users can type questions, retrieve relevant document chunks, and get answers generated by the LLM.

---

## Summary
The workflow:  
**Upload → Chunk → Embed → Store → Retrieve → Generate Answer**.  

This allows answering questions accurately based on the content of the uploaded documents rather than general knowledge.
